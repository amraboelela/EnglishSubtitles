#!/usr/bin/env python3
"""
Whisper Model Pruning for Turkish-Only Support

This script prunes the Whisper medium model to support only Turkish language,
removing unnecessary language-specific parameters and reducing model size.

Key pruning strategies:
1. Remove language tokens for other languages
2. Prune vocabulary to Turkish + English (for translation)
3. Remove language detection components
4. Keep only Turkish-relevant attention patterns

Usage:
    python prune_for_turkish.py
"""

import torch
import whisper
import numpy as np
from pathlib import Path
import json

def load_whisper_model():
    """Load the Whisper medium model"""
    print("ðŸ“¥ Loading Whisper medium model...")

    try:
        model = whisper.load_model("medium")

        total_params = sum(p.numel() for p in model.parameters())
        param_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)

        print(f"âœ… Original model: {total_params:,} parameters, {param_size:.1f} MB")
        return model

    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return None

def analyze_model_components(model):
    """Analyze model components to identify what can be pruned"""
    print("ðŸ” Analyzing model components for pruning opportunities...")

    state_dict = model.state_dict()

    # Categorize components
    encoder_params = []
    decoder_params = []
    embedding_params = []
    language_params = []

    total_size = 0
    component_sizes = {}

    for name, param in state_dict.items():
        size = param.numel() * param.element_size()
        total_size += size

        if 'encoder' in name:
            encoder_params.append(name)
            component_sizes['encoder'] = component_sizes.get('encoder', 0) + size
        elif 'decoder' in name:
            decoder_params.append(name)
            component_sizes['decoder'] = component_sizes.get('decoder', 0) + size
        elif 'token_embedding' in name or 'positional_embedding' in name:
            embedding_params.append(name)
            component_sizes['embeddings'] = component_sizes.get('embeddings', 0) + size
        else:
            language_params.append(name)
            component_sizes['other'] = component_sizes.get('other', 0) + size

    print(f"ðŸ“Š Model component analysis:")
    for component, size in component_sizes.items():
        percentage = (size / total_size) * 100
        print(f"   {component:12}: {size/(1024*1024):6.1f} MB ({percentage:5.1f}%)")

    return {
        'encoder': encoder_params,
        'decoder': decoder_params,
        'embeddings': embedding_params,
        'other': language_params
    }

def get_turkish_vocabulary_mask(model):
    """
    Create a mask for Turkish + English vocabulary
    Keep only tokens relevant for Turkish transcription and English translation
    """
    print("ðŸ‡¹ðŸ‡· Creating Turkish vocabulary mask...")

    # Get the tokenizer
    import tiktoken

    try:
        # Whisper uses a specific vocabulary
        tokenizer = tiktoken.get_encoding("gpt2")  # Whisper uses GPT-2 tokenizer base

        # Common Turkish characters and patterns
        turkish_chars = set("abcÃ§defgÄŸhÄ±ijklmnoÃ¶pqrsÅŸtuÃ¼vwxyzABCÃ‡DEFGÄžHIÄ°JKLMNOÃ–PQRSÅžTUÃœVWXYZ")
        english_chars = set("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ")
        numbers = set("0123456789")
        punctuation = set(".,!?;:()[]{}\"'-/ ")

        # Combine all allowed characters
        allowed_chars = turkish_chars | english_chars | numbers | punctuation

        # Get vocabulary size
        vocab_size = model.dims.n_vocab

        # Create mask (True = keep token, False = remove token)
        vocab_mask = np.zeros(vocab_size, dtype=bool)

        # Always keep special tokens (first ~300 tokens are usually special)
        vocab_mask[:300] = True

        # Check each token for Turkish/English relevance
        kept_tokens = 300
        for token_id in range(300, vocab_size):
            try:
                # Try to decode the token
                token_bytes = bytes([token_id])
                try:
                    token_str = token_bytes.decode('utf-8')
                    # Keep if contains Turkish/English characters
                    if any(char in allowed_chars for char in token_str):
                        vocab_mask[token_id] = True
                        kept_tokens += 1
                except:
                    # If can't decode, it's probably not text
                    pass
            except:
                pass

        compression_ratio = vocab_size / kept_tokens
        print(f"âœ“ Vocabulary pruning:")
        print(f"   Original vocab size: {vocab_size:,}")
        print(f"   Pruned vocab size: {kept_tokens:,}")
        print(f"   Compression ratio: {compression_ratio:.1f}x")

        return vocab_mask

    except Exception as e:
        print(f"âš ï¸  Vocabulary analysis failed: {e}")
        # Return full mask (keep everything) as fallback
        return np.ones(model.dims.n_vocab, dtype=bool)

def prune_embedding_layers(model, vocab_mask):
    """
    Prune embedding layers to remove unused vocabulary entries
    """
    print("âœ‚ï¸  Pruning embedding layers...")

    state_dict = model.state_dict()
    pruned_params = {}
    total_savings = 0

    for name, param in state_dict.items():
        if 'token_embedding' in name and param.dim() >= 2:
            # This is likely the token embedding matrix
            original_size = param.numel() * param.element_size()

            # Keep only rows corresponding to kept vocabulary
            if param.shape[0] == len(vocab_mask):
                pruned_param = param[vocab_mask]
                pruned_params[name] = pruned_param

                new_size = pruned_param.numel() * pruned_param.element_size()
                savings = original_size - new_size
                total_savings += savings

                print(f"   âœ“ {name}: {param.shape} -> {pruned_param.shape}")
                print(f"     Size: {original_size/(1024*1024):.1f}MB -> {new_size/(1024*1024):.1f}MB")
            else:
                pruned_params[name] = param
        else:
            pruned_params[name] = param

    print(f"ðŸ’¾ Total embedding savings: {total_savings/(1024*1024):.1f} MB")
    return pruned_params

def prune_language_specific_components(state_dict):
    """
    Remove or reduce language-specific components
    """
    print("ðŸ—‚ï¸  Pruning language-specific components...")

    pruned_dict = {}
    total_savings = 0

    for name, param in state_dict.items():
        original_size = param.numel() * param.element_size()

        # Skip language detection related parameters (if any)
        if any(skip_pattern in name for skip_pattern in ['language_head', 'lang_detect']):
            print(f"   ðŸ—‘ï¸  Removed: {name} ({original_size/(1024*1024):.1f}MB)")
            total_savings += original_size
            continue

        # For attention layers, we could prune heads but it's complex
        # For now, keep all attention layers intact for stability
        pruned_dict[name] = param

    if total_savings > 0:
        print(f"ðŸ’¾ Language-specific pruning savings: {total_savings/(1024*1024):.1f} MB")
    else:
        print("â„¹ï¸  No obvious language-specific components found to prune")

    return pruned_dict

def apply_structural_pruning(state_dict, prune_ratio=0.1):
    """
    Apply structural pruning to remove least important weights
    """
    print(f"âœ‚ï¸  Applying structural pruning ({prune_ratio*100:.0f}% of weights)...")

    pruned_dict = {}
    total_original_size = 0
    total_pruned_size = 0

    for name, param in state_dict.items():
        original_size = param.numel() * param.element_size()
        total_original_size += original_size

        if param.dim() >= 2 and 'ln' not in name and 'norm' not in name:
            # Apply magnitude-based pruning to weight matrices
            # Keep layer norms intact for stability

            # Calculate magnitude-based threshold
            param_abs = torch.abs(param)
            threshold = torch.quantile(param_abs.flatten(), prune_ratio)

            # Create mask for weights to keep
            mask = param_abs >= threshold

            # Apply mask (set pruned weights to 0)
            pruned_param = param * mask.float()
            pruned_dict[name] = pruned_param

            # Calculate actual sparsity achieved
            sparsity = (mask == 0).float().mean().item()
            new_size = original_size  # Size doesn't change with sparse representation

            if sparsity > 0.05:  # Only report if significant pruning
                print(f"   âœ“ {name}: {sparsity*100:.1f}% sparsity")
        else:
            # Keep bias, layer norms, etc. intact
            pruned_dict[name] = param

        total_pruned_size += param.numel() * param.element_size()

    return pruned_dict

def save_pruned_model(pruned_state, output_path):
    """Save the pruned model"""
    print(f"ðŸ’¾ Saving pruned model to: {output_path}")

    try:
        # Save with metadata about pruning
        save_data = {
            'model_state_dict': pruned_state,
            'pruning_info': {
                'target_language': 'turkish',
                'pruning_methods': ['vocabulary', 'structural'],
                'version': '1.0'
            }
        }

        torch.save(save_data, output_path)

        file_size = Path(output_path).stat().st_size / (1024 * 1024)
        print(f"âœ… Pruned model saved!")
        print(f"   File size: {file_size:.1f} MB")

        return file_size

    except Exception as e:
        print(f"âŒ Error saving pruned model: {e}")
        return 0

def main():
    """Main pruning workflow"""
    print("ðŸš€ Whisper Model Pruning for Turkish-Only Support")
    print("=" * 55)

    # Load original model
    model = load_whisper_model()
    if not model:
        return

    # Analyze model structure
    components = analyze_model_components(model)

    # Get Turkish vocabulary mask
    vocab_mask = get_turkish_vocabulary_mask(model)

    # Start with original state dict
    state_dict = model.state_dict()

    # Apply pruning steps
    print(f"\nðŸ”§ Applying pruning transformations...")

    # 1. Prune embeddings for Turkish vocabulary
    pruned_dict = prune_embedding_layers(model, vocab_mask)

    # 2. Remove language-specific components
    pruned_dict = prune_language_specific_components(pruned_dict)

    # 3. Apply structural pruning (optional)
    # pruned_dict = apply_structural_pruning(pruned_dict, prune_ratio=0.05)

    # Save pruned model
    output_path = "whisper_medium_turkish_pruned.pth"
    pruned_size = save_pruned_model(pruned_dict, output_path)

    # Compare with original
    original_size = 1457  # MB
    if pruned_size > 0:
        compression = original_size / pruned_size
        savings = original_size - pruned_size

        print(f"\nðŸ“Š PRUNING RESULTS:")
        print(f"   Original model: {original_size} MB")
        print(f"   Pruned model:   {pruned_size:.1f} MB")
        print(f"   Compression:    {compression:.1f}x")
        print(f"   Space saved:    {savings:.1f} MB")

    print(f"\nâœ… Turkish-only pruning completed!")
    print(f"ðŸŽ¯ Next: Test with Turkish audio to verify quality")

if __name__ == "__main__":
    main()